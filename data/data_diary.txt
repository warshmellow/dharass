Data Diary

March 2, 2015
A Bernoulli Naive Bayes trained on split of a ~450 tweet dataset consisting
of ~150 femfreq's hand classified harassment tweets with ~300 classified by
the development team, and NO cleaning of text, gives the following 
classification report:
(We use train_ and test_02032015.csv)

The cross validated precision, recall, f1-score, and support are as follows:
precision    recall  f1-score   support
1       0.82      0.93      0.87        30
1       0.90      0.87      0.88        30
1       0.73      0.89      0.80        27
1       0.77      0.88      0.82        34
1       0.76      0.94      0.84        31

The test set such scores are:
1       0.76      0.96      0.85        27

This model is the baseline model. It is generated by script 
create_bnb_model_02032015.py.

March 4, 2015
Created F1-Score, Precision, Recall vs Proportion of Positive Class on a
validation set to check sensitivity of above model to class imbalance.
See "exploration 3 march 2015.ipynb" for charts. Suggests that the model is
sensitive up to 5%, in terms of F1-score. After 5%, F1-score increases above
0.5. And we use F1-score because it is unclear whether precision or recall is
more important, so let's weight them equally.

March 9, 2015
If we put an additional requirement on the classifier: we need to remove all screen names, starting with @, we remove all the data about who the tweet was
sent to, i.e., if it was sent to a particular woman! It is unclear whether this is important. But according to preliminary tests, it is! Cf. the following results.
Use the same train test data as create_bnb_model_02032015.py, and same random states.
Tfidf and Multinomial Naive Bayes, keep screen names
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.93      0.87      0.90        30
          1       1.00      0.70      0.82        30
          1       0.84      0.78      0.81        27
          1       0.92      0.71      0.80        34
          1       0.92      0.77      0.84        31

Classification Report on Test Set
          1       0.96      0.85      0.90        27

Tfidf and Multinomial Naive Bayes, remove screen names
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.96      0.83      0.89        30
          1       1.00      0.73      0.85        30
          1       0.77      0.74      0.75        27
          1       0.84      0.62      0.71        34
          1       0.93      0.81      0.86        31

Classification Report on Test Set
          1       0.69      0.93      0.79        27

Binary Count vectorization and Bernouilli Naive Bayes, remove screen names
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.84      0.90      0.87        30
          1       0.88      0.77      0.82        30
          1       0.75      0.89      0.81        27
          1       0.82      0.91      0.86        34
          1       0.77      0.87      0.82        31

Classification Report on Test Set
          1       0.62      0.96      0.75        27

Multinomial Count vectorization and Multinomial Naive Bayes, remove screen names
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.84      0.90      0.87        30
          1       0.88      0.77      0.82        30
          1       0.75      0.89      0.81        27
          1       0.82      0.91      0.86        34
          1       0.77      0.87      0.82        31

Classification Report on Test Set
          1       0.62      0.96      0.75        27

Random Forest needs dense matrix, so we shouldn't use it for now.

Tfidf vectorization and L1-reg logistic regression (default otherwise), 
remove screen
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.90      0.87      0.88        30
          1       0.77      0.77      0.77        30
          1       0.86      0.89      0.87        27
          1       0.84      0.62      0.71        34
          1       0.88      0.94      0.91        31

Classification Report on Test Set
          1       0.88      0.78      0.82        27

Tf vectorization (to normalize) and L1-reg logistic regression (default ow),
remove screen
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.90      0.93      0.92        30
          1       0.79      0.77      0.78        30
          1       0.86      0.89      0.87        27
          1       0.82      0.53      0.64        34
          1       0.88      0.90      0.89        31

Classification Report on Test Set
          1       0.80      0.74      0.77        27

Tfidf vectorization and SVM with rbf kernel (default ow), remove screen
(Abysmal: it can't even find the positive class)
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.00      0.00      0.00        30
          1       0.00      0.00      0.00        30
          1       0.00      0.00      0.00        27
          1       0.00      0.00      0.00        34
          1       0.00      0.00      0.00        31

Classification Report on Test Set
          1       0.00      0.00      0.00        27

In fact, trying combinations of Count and TF-IDF, n-gram range 1-3, 
using stop words, and using Naive Bayes and L1-Logistic Regression, we find
test and train F1 right around 0.8, with no real difference between any combo!
We should try more data. We should try spellchecking. Recall, I still can't
classify "I love chocolate" correctly!
Stick with 1-grams, TF-IDF, and Naive Bayes for now.

With a new small dataset (~350) of @femfreq's mentions from March 9, 2015, there
is about 13% harassment. We are still wondering if it is a rare event for
her and other women.

We combine the above dataset with previous train_02032015.csv and test on test_02032015.csv. We achieve the following 

Next steps: 
Study the examples misclassified by BernoulliNB: BernoulliNB with or without stop words removed is the one to beat
Lasso Logistic Regression and Linear SVM both with stop words removed seem promising too.
Try tuning their complexity parameters.
Counts and MultinomialNB, anonymized.
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.76      0.62      0.68        47
          1       0.81      0.74      0.78        35
          1       0.75      0.82      0.78        33
          1       0.82      0.82      0.82        38
          1       0.68      0.58      0.63        45

Classification Report on Test Set
          1       0.85      0.85      0.85        27

Avg CV F1 Score: 0.738000 

Counts, remove stopwords, MultinomialNB, anonymized.
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.70      0.68      0.69        47
          1       0.74      0.80      0.77        35
          1       0.68      0.79      0.73        33
          1       0.78      0.76      0.77        38
          1       0.66      0.56      0.60        45

Classification Report on Test Set
          1       0.79      0.85      0.82        27

Avg CV F1 Score: 0.738000

Tfidf, remove stopwords, L1-Logistic Regression (default), anonymized
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.95      0.45      0.61        47
          1       0.95      0.54      0.69        35
          1       0.95      0.64      0.76        33
          1       1.00      0.58      0.73        38
          1       1.00      0.44      0.62        45

Classification Report on Test Set
             precision    recall  f1-score   support
          1       1.00      0.70      0.83        27

Avg CV F1 Score: 0.682000

Tfidf, remove stopwords, Linear SVM (default), anonymized
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.87      0.57      0.69        47
          1       0.81      0.60      0.69        35
          1       0.75      0.73      0.74        33
          1       0.93      0.66      0.77        38
          1       0.80      0.44      0.57        45

Classification Report on Test Set
          1       0.92      0.85      0.88        27

Avg CV F1 Score: 0.692000

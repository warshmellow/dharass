Data Diary

March 2, 2015
A Bernoulli Naive Bayes trained on split of a ~450 tweet dataset consisting
of ~150 femfreq's hand classified harassment tweets with ~300 classified by
the development team, and NO cleaning of text, gives the following 
classification report:
(We use train_ and test_02032015.csv)

The cross validated precision, recall, f1-score, and support are as follows:
precision    recall  f1-score   support
1       0.82      0.93      0.87        30
1       0.90      0.87      0.88        30
1       0.73      0.89      0.80        27
1       0.77      0.88      0.82        34
1       0.76      0.94      0.84        31

The test set such scores are:
1       0.76      0.96      0.85        27

This model is the baseline model. It is generated by script 
create_bnb_model_02032015.py.

March 4, 2015
Created F1-Score, Precision, Recall vs Proportion of Positive Class on a
validation set to check sensitivity of above model to class imbalance.
See "exploration 3 march 2015.ipynb" for charts. Suggests that the model is
sensitive up to 5%, in terms of F1-score. After 5%, F1-score increases above
0.5. And we use F1-score because it is unclear whether precision or recall is
more important, so let's weight them equally.

March 9, 2015
If we put an additional requirement on the classifier: we need to remove all screen names, starting with @, we remove all the data about who the tweet was
sent to, i.e., if it was sent to a particular woman! It is unclear whether this is important. But according to preliminary tests, it is! Cf. the following results.
Use the same train test data as create_bnb_model_02032015.py, and same random states.
Tfidf and Multinomial Naive Bayes, keep screen names
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.93      0.87      0.90        30
          1       1.00      0.70      0.82        30
          1       0.84      0.78      0.81        27
          1       0.92      0.71      0.80        34
          1       0.92      0.77      0.84        31

Classification Report on Test Set
          1       0.96      0.85      0.90        27

Tfidf and Multinomial Naive Bayes, remove screen names
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.96      0.83      0.89        30
          1       1.00      0.73      0.85        30
          1       0.77      0.74      0.75        27
          1       0.84      0.62      0.71        34
          1       0.93      0.81      0.86        31

Classification Report on Test Set
          1       0.69      0.93      0.79        27

Binary Count vectorization and Bernouilli Naive Bayes, remove screen names
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.84      0.90      0.87        30
          1       0.88      0.77      0.82        30
          1       0.75      0.89      0.81        27
          1       0.82      0.91      0.86        34
          1       0.77      0.87      0.82        31

Classification Report on Test Set
          1       0.62      0.96      0.75        27

Multinomial Count vectorization and Multinomial Naive Bayes, remove screen names
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.84      0.90      0.87        30
          1       0.88      0.77      0.82        30
          1       0.75      0.89      0.81        27
          1       0.82      0.91      0.86        34
          1       0.77      0.87      0.82        31

Classification Report on Test Set
          1       0.62      0.96      0.75        27

Random Forest needs dense matrix, so we shouldn't use it for now.

Tfidf vectorization and L1-reg logistic regression (default otherwise), 
remove screen
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.90      0.87      0.88        30
          1       0.77      0.77      0.77        30
          1       0.86      0.89      0.87        27
          1       0.84      0.62      0.71        34
          1       0.88      0.94      0.91        31

Classification Report on Test Set
          1       0.88      0.78      0.82        27

Tf vectorization (to normalize) and L1-reg logistic regression (default ow),
remove screen
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.90      0.93      0.92        30
          1       0.79      0.77      0.78        30
          1       0.86      0.89      0.87        27
          1       0.82      0.53      0.64        34
          1       0.88      0.90      0.89        31

Classification Report on Test Set
          1       0.80      0.74      0.77        27

Tfidf vectorization and SVM with rbf kernel (default ow), remove screen
(Abysmal: it can't even find the positive class)
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.00      0.00      0.00        30
          1       0.00      0.00      0.00        30
          1       0.00      0.00      0.00        27
          1       0.00      0.00      0.00        34
          1       0.00      0.00      0.00        31

Classification Report on Test Set
          1       0.00      0.00      0.00        27

In fact, trying combinations of Count and TF-IDF, n-gram range 1-3, 
using stop words, and using Naive Bayes and L1-Logistic Regression, we find
test and train F1 right around 0.8, with no real difference between any combo!
We should try more data. We should try spellchecking. Recall, I still can't
classify "I love chocolate" correctly!
Stick with 1-grams, TF-IDF, and Naive Bayes for now.

With a new small dataset (~350) of @femfreq's mentions from March 9, 2015, there
is about 13% harassment. We are still wondering if it is a rare event for
her and other women.

We combine the above dataset with previous train_02032015.csv, into 
femfreq_mentions03092015_drop_dup_labeled_anon.csv, 
and test on test_02032015.csv. We achieve the following 

Next steps: 
Study the examples misclassified by BernoulliNB: BernoulliNB with or without stop words removed is the one to beat
Lasso Logistic Regression and Linear SVM both with stop words removed seem promising too.
Try tuning their complexity parameters.
Counts and MultinomialNB, anonymized.
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.76      0.62      0.68        47
          1       0.81      0.74      0.78        35
          1       0.75      0.82      0.78        33
          1       0.82      0.82      0.82        38
          1       0.68      0.58      0.63        45

Classification Report on Test Set
          1       0.85      0.85      0.85        27

Avg CV F1 Score: 0.738000 

Counts, remove stopwords, MultinomialNB, anonymized.
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.70      0.68      0.69        47
          1       0.74      0.80      0.77        35
          1       0.68      0.79      0.73        33
          1       0.78      0.76      0.77        38
          1       0.66      0.56      0.60        45

Classification Report on Test Set
          1       0.79      0.85      0.82        27

Avg CV F1 Score: 0.738000

Tfidf, remove stopwords, L1-Logistic Regression (default), anonymized
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.95      0.45      0.61        47
          1       0.95      0.54      0.69        35
          1       0.95      0.64      0.76        33
          1       1.00      0.58      0.73        38
          1       1.00      0.44      0.62        45

Classification Report on Test Set
             precision    recall  f1-score   support
          1       1.00      0.70      0.83        27

Avg CV F1 Score: 0.682000

Tfidf, remove stopwords, Linear SVM (default), anonymized
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.87      0.57      0.69        47
          1       0.81      0.60      0.69        35
          1       0.75      0.73      0.74        33
          1       0.93      0.66      0.77        38
          1       0.80      0.44      0.57        45

Classification Report on Test Set
          1       0.92      0.85      0.88        27

Avg CV F1 Score: 0.692000

March 10, 2015
# Main issues with working with Tweets:
# 1. harassment vs literally every other topic (and the latter expands across time) 
# - so need an updating model (online or period batch update)
# 2. data sparsity - so little data per document so very sensitive to misspellings and appearances of single words. These
# problems are usually avoided if the text is long enough and has very few misspellings
# - so try reducing to a large text problem by 1. spelling correction and 2. append large text in "ground truth" source like
# wikipedia (try titles only?) and 3. use n-grams for n>=1

On training set femfreq_mentions03092015_drop_dup_labeled_anon.csv, 
run_kfold_and_print_f1_score(X, y, TfidfVectorizer(stop_words='english'), LogisticRegression('l1'), 1, 5, test)
achieves:
Avg F1 score: 0.682392
F1 score on Test Set
0.826086956522

The same, we use gridsearch to get the following:
Best estimator:  LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, penalty='l1', random_state=None, tol=0.0001)
Best score:  0.629783869441
Best params:  {'penalty': 'l1', 'C': 100.0}
Best estimator:  LogisticRegression(C=10000.0, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, penalty='l1',
          random_state=None, tol=0.0001)
Best score:  0.658603752661
Best params:  {'penalty': 'l1', 'C': 10000.0}
Best estimator:  LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, penalty='l1', random_state=None, tol=0.0001)
Best score:  0.622437024019
Best params:  {'penalty': 'l1', 'C': 100.0}
Best estimator:  LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, penalty='l1', random_state=None, tol=0.0001)
Best score:  0.638024563853
Best params:  {'penalty': 'l1', 'C': 100.0}
Best estimator:  LogisticRegression(C=1000.0, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, penalty='l1',
          random_state=None, tol=0.0001)
Best score:  0.681998731911
Best params:  {'penalty': 'l1', 'C': 1000.0}
F1 scores [0.72093023255813959, 0.6470588235294118, 0.71428571428571419, 0.79411764705882348, 0.6913580246913581]
Avg F1 score: 0.713550
F1 score on Test Set
0.909090909091

For linear svm (remove english stopwords, tfidf):
Best estimator:  LinearSVC(C=10.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
     random_state=None, tol=0.0001, verbose=0)
Best score:  0.622794724377
Best params:  {'C': 10.0}
Best estimator:  LinearSVC(C=100.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
     random_state=None, tol=0.0001, verbose=0)
Best score:  0.652357692302
Best params:  {'C': 100.0}
Best estimator:  LinearSVC(C=10.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
     random_state=None, tol=0.0001, verbose=0)
Best score:  0.614762832454
Best params:  {'C': 10.0}
Best estimator:  LinearSVC(C=100.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
     random_state=None, tol=0.0001, verbose=0)
Best score:  0.579774335042
Best params:  {'C': 100.0}
Best estimator:  LinearSVC(C=10.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
     random_state=None, tol=0.0001, verbose=0)
Best score:  0.670318460998
Best params:  {'C': 10.0}
F1 scores [0.77108433734939752, 0.6470588235294118, 0.74285714285714288, 0.78378378378378377, 0.60526315789473684]
Avg F1 score: 0.710009
F1 score on Test Set
0.872727272727

Multinomial Naive Bayes with CountVectorization, remove english stopwords:
F1 scores [0.68817204301075263, 0.73972602739726012, 0.73239436619718301, 0.77333333333333343, 0.60240963855421692]
Avg F1 score: 0.707207
F1 score on Test Set
0.821428571429

There doesn't seem to be any significant improvement over the Count Vectorized
Multinomial NB with removed english stopwords. Since Naive Bayes is the simplest,
we should stick with that one, and maybe tune Lasso (esp. wrt. Precision-Recall
and ROC curve), time permitting.


These are the mislabeled in my training set.
False positives: unfairly labeled nasty
  And they say we don't understand free speech.
...
 Holy fuck. Did you just dump that pile of horsecrap into a conversation? Get lost, we're done here.  
...
   Simple: don't be dishonest in order to make money or get publicity.
...
 What is a woman expected to do to avoid being harassed. Neither  nor  is "batshit insane".
...
  These are people who want to put their fingers in their ears and go LALALA GAMING IS FINE SHUT UP.
...
   ok then sorry I might have mis tweeted. SorryðŸ˜³
...
 Aren't you tired of rambling on with misinformation? You use the damsel in distress arguement, despite not having played them.
...
 !!! Also going to talk by  tonight, wish u were ere
...
   Link so I can try it on my own male characters? :3
...
 Damn, a WHOLE 43 followers. You're impressive. What're you pestering  for? Get a fucking blog, express yourself, cretin.
...
   if your little feelings get hurt by women describing their experiences you don't really qualify as a 'man'.
...
   Maybe instead of saying "it's just the internet", we make it so that everyone feels welcome online
...
  The person involved in the mass shooting was mentally ill and had a crap life, he was not sexist or misogynistic.
...
  (daily) rape and death threats are not a small thing and do not happen to everyone.
...

False Negatives: Nasties we didn't catch
  reliance on sexism stifles creation you say, explains why you have been unable to complete your video series.
...
 yeah I mean look at  she was never a 'gamer' as she put it
...
RT :        
Feminist attacking egalitarians are why they becâ€¦
...
how about you get cancer 
...
RT : You're pretty smart for a woman.  http://t.co/1FcFF48nQX
...
RT : You're pretty smart for a woman.  http://t.co/1FcFF48nQX
...
RT :  um you do realize that your a brand right not a feminist.
...
  You done yet?
...
Why are video games so sexist!? Tell me, ! You have all the answers, right!?
...
  It is not because you are a woman
...
  it's not something to be scared of. I get death threats and I play video games on the Internet to entertain people
...
RT : A gamers girls opinion on the Anita Sarkeesian controversy http://t.co/ipJVDvnoQo #anitasarkeesian #GamerGate 
...

And in our test set:
Also, quelle surprise, lots of retweets from threatened reactionaries, who would have thought!
True target:  0
...
  like a GGer told me at length recently "liberal arts is bullshit" and "critics should strive to be objective" 
lmao
True target:  0
...
   Typical of left wing extremists. Blocking their way to an echochamber. Ask  all about it.
True target:  0
...
 get cancer
True target:  1
...
 I hope every feminist has their head severed from their shoulders.
True target:  1
...
   Someone prints this shit on paper?
True target:  0
...
 If you need more evidence that YOU HAVEN'T GOT THE SLIGHTEST IDEA WHAT YOU'RE TALKING ABOUT, just ask.  
True target:  0
...
Why are video games so sexist!? Tell me, ! You have all the answers, right!?
True target:  1
...
 should be  or  LOL! I HATE FEMCUNTS!!
True target:  1
...
 Has  "toppled an entire industry"? Can't have been much of an industry. Got a news story on this?
True target:  0
...

In general, we have trouble with: some I just mislabeled or are hard to tell, some involve made up insults that aren't in any dictionary (What is a 'FEMCUNT'?), some are just mansplaining (which can't really be found by keyword), some rely on n-grams n>1 ("you're pretty smart for a woman"), some need context ("get cancer"), some are about corporal violence, which can be said in many ways ("I hope every feminist has their head severed from their shoulders.")

For an optimization, I might allow users to enter their own "hot" words or phrases.
This may be in a basic OR model (if contains hot word OR labeled 1 by Naive Bayes
model, block).

There doesn't seem to be noticeable improvement with 1,2,3 n-grams (in terms of F1-score)
It's like 1-2 point (out of 100) improvement, which is prolly not noticeable to users anyway.

This seems a bit similar, maybe it will help you just to see their results http://cs229.stanford.edu/proj2011/BovalTobin-FlameWarDetectionUsingNaiveBayes.pdf

From this paper, it seems like the main idea on improvements, in the paper, are that 1. moar data improves the model the best 2. frequency estimate extension (a semi-supervised technique), spelling correction, stemming, and correlating length of discussion were useless.

For now, we can leave the model as is, and build a user "hot" word/phrase model,
and roll in a semi-supervised NB.

March 17, 2015
We train a Multinomial Naive Bayes Model with labeled and unlabeled data, (remove stop words) train_03102015.csv and a full dump of unique tweets from the database (collected March 10 to March 17 from @femfreq's mentions). This full dump is 1994 unique tweets (discounting RT). We use OpenPR-NBEM semi supervised library from Rui Xia (http://msrt.njust.edu.cn/staff/rxia/). We take metrics
on test file test_03102015.csv.

The command and output is:
./nbem_ssl ../labeled_train_03102015.samp ../unlabeled_full_dump.samp nbem_ssl_train_03102015.mod
Loading training data...
Loading unlabeled data...

EM learning...
Initial loglikelihood: -150581

Iter: 1
Loglikelihood: -145077, increasing 0.0365506

Iter: 2
Loglikelihood: -144773, increasing 0.0020932

Iter: 3
Loglikelihood: -144656, increasing 0.000809128

Iter: 4
Loglikelihood: -144608, increasing 0.000332178

Iter: 5
Loglikelihood: -144586, increasing 0.000154132

Iter: 6
Loglikelihood: -144575, increasing 7.70549e-05
Reach convergence!
Saving model...

./nb_classify ../labeled_test_03102015.samp nbem_ssl_train_03102015.mod ../test_03102015_pred.txt
Loading model...
Classifying test file...
Accuracy: 0.880435

 precision    recall  f1-score   support
          1       0.81      0.78      0.79        27

This training doesn't seem to improve it that much. Oh well. Maybe more data?

On the training data set:
             precision    recall  f1-score   support
          1       0.98      0.84      0.90       198
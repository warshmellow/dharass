Data Diary

March 2, 2015
A Bernoulli Naive Bayes trained on split of a ~450 tweet dataset consisting
of ~150 femfreq's hand classified harassment tweets with ~300 classified by
the development team, and NO cleaning of text, gives the following 
classification report:
(We use train_ and test_02032015.csv)

The cross validated precision, recall, f1-score, and support are as follows:
precision    recall  f1-score   support
1       0.82      0.93      0.87        30
1       0.90      0.87      0.88        30
1       0.73      0.89      0.80        27
1       0.77      0.88      0.82        34
1       0.76      0.94      0.84        31

The test set such scores are:
1       0.76      0.96      0.85        27

This model is the baseline model. It is generated by script 
create_bnb_model_02032015.py.

March 4, 2015
Created F1-Score, Precision, Recall vs Proportion of Positive Class on a
validation set to check sensitivity of above model to class imbalance.
See "exploration 3 march 2015.ipynb" for charts. Suggests that the model is
sensitive up to 5%, in terms of F1-score. After 5%, F1-score increases above
0.5. And we use F1-score because it is unclear whether precision or recall is
more important, so let's weight them equally.

March 9, 2015
If we put an additional requirement on the classifier: we need to remove all screen names, starting with @, we remove all the data about who the tweet was
sent to, i.e., if it was sent to a particular woman! It is unclear whether this is important. But according to preliminary tests, it is! Cf. the following results.
Use the same train test data as create_bnb_model_02032015.py, and same random states.
Tfidf and Multinomial Naive Bayes, keep screen names
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.93      0.87      0.90        30
          1       1.00      0.70      0.82        30
          1       0.84      0.78      0.81        27
          1       0.92      0.71      0.80        34
          1       0.92      0.77      0.84        31

Classification Report on Test Set
          1       0.96      0.85      0.90        27

Tfidf and Multinomial Naive Bayes, remove screen names
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.96      0.83      0.89        30
          1       1.00      0.73      0.85        30
          1       0.77      0.74      0.75        27
          1       0.84      0.62      0.71        34
          1       0.93      0.81      0.86        31

Classification Report on Test Set
          1       0.69      0.93      0.79        27

Binary Count vectorization and Bernouilli Naive Bayes, remove screen names
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.84      0.90      0.87        30
          1       0.88      0.77      0.82        30
          1       0.75      0.89      0.81        27
          1       0.82      0.91      0.86        34
          1       0.77      0.87      0.82        31

Classification Report on Test Set
          1       0.62      0.96      0.75        27

Multinomial Count vectorization and Multinomial Naive Bayes, remove screen names
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.84      0.90      0.87        30
          1       0.88      0.77      0.82        30
          1       0.75      0.89      0.81        27
          1       0.82      0.91      0.86        34
          1       0.77      0.87      0.82        31

Classification Report on Test Set
          1       0.62      0.96      0.75        27

Random Forest needs dense matrix, so we shouldn't use it for now.

Tfidf vectorization and L1-reg logistic regression (default otherwise), 
remove screen
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.90      0.87      0.88        30
          1       0.77      0.77      0.77        30
          1       0.86      0.89      0.87        27
          1       0.84      0.62      0.71        34
          1       0.88      0.94      0.91        31

Classification Report on Test Set
          1       0.88      0.78      0.82        27

Tf vectorization (to normalize) and L1-reg logistic regression (default ow),
remove screen
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.90      0.93      0.92        30
          1       0.79      0.77      0.78        30
          1       0.86      0.89      0.87        27
          1       0.82      0.53      0.64        34
          1       0.88      0.90      0.89        31

Classification Report on Test Set
          1       0.80      0.74      0.77        27

Tfidf vectorization and SVM with rbf kernel (default ow), remove screen
(Abysmal: it can't even find the positive class)
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.00      0.00      0.00        30
          1       0.00      0.00      0.00        30
          1       0.00      0.00      0.00        27
          1       0.00      0.00      0.00        34
          1       0.00      0.00      0.00        31

Classification Report on Test Set
          1       0.00      0.00      0.00        27

In fact, trying combinations of Count and TF-IDF, n-gram range 1-3, 
using stop words, and using Naive Bayes and L1-Logistic Regression, we find
test and train F1 right around 0.8, with no real difference between any combo!
We should try more data. We should try spellchecking. Recall, I still can't
classify "I love chocolate" correctly!
Stick with 1-grams, TF-IDF, and Naive Bayes for now.

With a new small dataset (~350) of @femfreq's mentions from March 9, 2015, there
is about 13% harassment. We are still wondering if it is a rare event for
her and other women.

We combine the above dataset with previous train_02032015.csv, into 
femfreq_mentions03092015_drop_dup_labeled_anon.csv, 
and test on test_02032015.csv. We achieve the following 

Next steps: 
Study the examples misclassified by BernoulliNB: BernoulliNB with or without stop words removed is the one to beat
Lasso Logistic Regression and Linear SVM both with stop words removed seem promising too.
Try tuning their complexity parameters.
Counts and MultinomialNB, anonymized.
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.76      0.62      0.68        47
          1       0.81      0.74      0.78        35
          1       0.75      0.82      0.78        33
          1       0.82      0.82      0.82        38
          1       0.68      0.58      0.63        45

Classification Report on Test Set
          1       0.85      0.85      0.85        27

Avg CV F1 Score: 0.738000 

Counts, remove stopwords, MultinomialNB, anonymized.
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.70      0.68      0.69        47
          1       0.74      0.80      0.77        35
          1       0.68      0.79      0.73        33
          1       0.78      0.76      0.77        38
          1       0.66      0.56      0.60        45

Classification Report on Test Set
          1       0.79      0.85      0.82        27

Avg CV F1 Score: 0.738000

Tfidf, remove stopwords, L1-Logistic Regression (default), anonymized
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.95      0.45      0.61        47
          1       0.95      0.54      0.69        35
          1       0.95      0.64      0.76        33
          1       1.00      0.58      0.73        38
          1       1.00      0.44      0.62        45

Classification Report on Test Set
             precision    recall  f1-score   support
          1       1.00      0.70      0.83        27

Avg CV F1 Score: 0.682000

Tfidf, remove stopwords, Linear SVM (default), anonymized
Classification Report for CV Fold
             precision    recall  f1-score   support
          1       0.87      0.57      0.69        47
          1       0.81      0.60      0.69        35
          1       0.75      0.73      0.74        33
          1       0.93      0.66      0.77        38
          1       0.80      0.44      0.57        45

Classification Report on Test Set
          1       0.92      0.85      0.88        27

Avg CV F1 Score: 0.692000

March 10, 2015
# Main issues with working with Tweets:
# 1. harassment vs literally every other topic (and the latter expands across time) 
# - so need an updating model (online or period batch update)
# 2. data sparsity - so little data per document so very sensitive to misspellings and appearances of single words. These
# problems are usually avoided if the text is long enough and has very few misspellings
# - so try reducing to a large text problem by 1. spelling correction and 2. append large text in "ground truth" source like
# wikipedia (try titles only?) and 3. use n-grams for n>=1

On training set femfreq_mentions03092015_drop_dup_labeled_anon.csv, 
run_kfold_and_print_f1_score(X, y, TfidfVectorizer(stop_words='english'), LogisticRegression('l1'), 1, 5, test)
achieves:
Avg F1 score: 0.682392
F1 score on Test Set
0.826086956522

The same, we use gridsearch to get the following:
Best estimator:  LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, penalty='l1', random_state=None, tol=0.0001)
Best score:  0.629783869441
Best params:  {'penalty': 'l1', 'C': 100.0}
Best estimator:  LogisticRegression(C=10000.0, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, penalty='l1',
          random_state=None, tol=0.0001)
Best score:  0.658603752661
Best params:  {'penalty': 'l1', 'C': 10000.0}
Best estimator:  LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, penalty='l1', random_state=None, tol=0.0001)
Best score:  0.622437024019
Best params:  {'penalty': 'l1', 'C': 100.0}
Best estimator:  LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, penalty='l1', random_state=None, tol=0.0001)
Best score:  0.638024563853
Best params:  {'penalty': 'l1', 'C': 100.0}
Best estimator:  LogisticRegression(C=1000.0, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, penalty='l1',
          random_state=None, tol=0.0001)
Best score:  0.681998731911
Best params:  {'penalty': 'l1', 'C': 1000.0}
F1 scores [0.72093023255813959, 0.6470588235294118, 0.71428571428571419, 0.79411764705882348, 0.6913580246913581]
Avg F1 score: 0.713550
F1 score on Test Set
0.909090909091

For linear svm (remove english stopwords, tfidf):
Best estimator:  LinearSVC(C=10.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
     random_state=None, tol=0.0001, verbose=0)
Best score:  0.622794724377
Best params:  {'C': 10.0}
Best estimator:  LinearSVC(C=100.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
     random_state=None, tol=0.0001, verbose=0)
Best score:  0.652357692302
Best params:  {'C': 100.0}
Best estimator:  LinearSVC(C=10.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
     random_state=None, tol=0.0001, verbose=0)
Best score:  0.614762832454
Best params:  {'C': 10.0}
Best estimator:  LinearSVC(C=100.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
     random_state=None, tol=0.0001, verbose=0)
Best score:  0.579774335042
Best params:  {'C': 100.0}
Best estimator:  LinearSVC(C=10.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
     random_state=None, tol=0.0001, verbose=0)
Best score:  0.670318460998
Best params:  {'C': 10.0}
F1 scores [0.77108433734939752, 0.6470588235294118, 0.74285714285714288, 0.78378378378378377, 0.60526315789473684]
Avg F1 score: 0.710009
F1 score on Test Set
0.872727272727

Multinomial Naive Bayes with CountVectorization, remove english stopwords:
F1 scores [0.68817204301075263, 0.73972602739726012, 0.73239436619718301, 0.77333333333333343, 0.60240963855421692]
Avg F1 score: 0.707207
F1 score on Test Set
0.821428571429

There doesn't seem to be any significant improvement over the Count Vectorized
Multinomial NB with removed english stopwords. Since Naive Bayes is the simplest,
we should stick with that one, and maybe tune Lasso (esp. wrt. Precision-Recall
and ROC curve), time permitting.
